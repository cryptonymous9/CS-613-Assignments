{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Approach: (n-Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and MLE Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Data cleaning and sentence tokenization\n",
    "\n",
    "def preprocess(fp):\n",
    "    \n",
    "    f = codecs.open(fp, 'r', 'UTF-8')\n",
    "    raw_text = f.read()\n",
    "\n",
    "    raw_text = raw_text.replace(\"\\r\\n\", \"\")\n",
    "    raw_text = raw_text.replace(\"SPEECH\", \"\")\n",
    "    raw_text = raw_text.replace(\"\\'\", \"\")\n",
    "    raw_text = raw_text.replace(\"-\", \"\")\n",
    "    raw_text = raw_text.replace(\"$\", \"\")\n",
    "    raw_text = raw_text.replace(\",\", \"\")\n",
    "    raw_text = raw_text.replace(\":\", \"\")\n",
    "\n",
    "    raw_text = re.sub('[0-9]', r'', raw_text)\n",
    "\n",
    "    text = raw_text\n",
    "    \n",
    "    sent_list = sent_tokenize(text)\n",
    "    new_list = []\n",
    "    \n",
    "    for sent in sent_list:\n",
    "        sent = re.sub('[\\\"]', r'',sent)\n",
    "        sent = re.sub('[.]', r'',sent)\n",
    "        sent = re.sub('[:]', r'',sent)\n",
    "        sent = re.sub('[;]', r'',sent)\n",
    "        sent = re.sub(r'\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]', r'\\1', sent)\n",
    "        new_list.append('<s> ' + sent.lower() + ' </s>')\n",
    "        \n",
    "    return new_list\n",
    "\n",
    "\n",
    "# Computing freq counts for MLE\n",
    "def gen_ngrams(sent_list, N):\n",
    "\n",
    "    ngrams = []\n",
    "    for sent in sent_list:\n",
    "        tokens = sent.split()\n",
    "        for i in range(len(tokens)-N+1):\n",
    "            ngrams.append(tokens[i:i+N])\n",
    "\n",
    "    freq_count = {}\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        token_seq  = ' '.join(ngram[:-1])\n",
    "        last_token = str(ngram[-1])\n",
    "\n",
    "        if token_seq not in freq_count:\n",
    "            freq_count[token_seq] = {};\n",
    "        \n",
    "        if last_token not in freq_count[token_seq]:\n",
    "            freq_count[token_seq][last_token] = 0;\n",
    "\n",
    "        freq_count[token_seq][last_token] += 1;\n",
    "        \n",
    "    return freq_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def predict_word(text, N, freq_count):\n",
    "    \n",
    "    token_seq = ' '.join(text.split()[-(N-1):])\n",
    "    if N!=1:\n",
    "        choices = freq_count[token_seq].items()\n",
    "    else:\n",
    "        choices = freq_count[''].items()\n",
    "    pvals=[]\n",
    "    key_words=[]\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    \n",
    "    for key,values in choices:\n",
    "        key_words.append(key)\n",
    "        pvals.append(values/total)\n",
    "        \n",
    "    r = np.random.multinomial(3, pvals, size=None)\n",
    "    req_index = np.argmax(r)\n",
    "    choice = key_words[req_index]\n",
    "    \n",
    "    return choice\n",
    "   \n",
    "def generator(N, freq_count, start_seq):\n",
    "    start_tag_list=[]\n",
    "    \n",
    "    for i in freq_count.keys():\n",
    "        a = i.split()\n",
    "        if N!=1 and a[0]=='<s>':\n",
    "            start_tag_list.append(i)\n",
    "    \n",
    "    if(start_seq is None) and N!=1: \n",
    "        start_seq = random.choice(start_tag_list);\n",
    "    elif(start_seq is None) and N==1:\n",
    "        start_seq=\"<s>\"\n",
    "    rand_text = start_seq.lower();\n",
    "\n",
    "    sentences = 0;\n",
    "    \n",
    "    next_word = ''\n",
    "    \n",
    "    while next_word!= '</s>':\n",
    "        next_word = predict_word(rand_text, N, freq_count)\n",
    "        rand_text += ' ' + next_word\n",
    "        \n",
    "    return rand_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Generate texts using user defined value of n for n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input which n-gram model you want to use for generating sentence.\n",
      "Example: 1 for Unigram, 2 for Bigram, 3 for Trigram, 4 for quadgrams\n",
      "\n",
      "4\n",
      "\n",
      "Generating Text ...\n",
      "\n",
      "<s> i get sued all the time okay </s>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input which n-gram model you want to use for generating sentence.\")\n",
    "print(\"Example: 1 for Unigram, 2 for Bigram, 3 for Trigram, 4 for quadgrams\\n\")\n",
    "\n",
    "N= int(input())\n",
    "\n",
    "print(\"\\nGenerating Text ...\\n\")\n",
    "sent_list = preprocess('speech.txt')\n",
    "train_sent_list, test_sent_list = sent_list[:1000],sent_list[1000:]\n",
    "freq_count = gen_ngrams(train_sent_list, N)\n",
    "print(generator(N, freq_count, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Texts using n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> thank you can tell me </s>\n",
      "<s> a lot of the people </s>\n",
      "<s> we cant believe me if you look at what are going to be gone very strongly its south korea increases its just not going to make western values that we donâ€™t know how we have to do it </s>\n",
      "<s> and i have to look at all heard that are easy to be very good to be a great respect stupid are closing up and i will not help us </s>\n",
      "<s> i have a very disappointed by the ones </s>\n"
     ]
    }
   ],
   "source": [
    "# bigram is the below case\n",
    "for i in range(5):\n",
    "    print(generator(N, freq_count, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readibility: \n",
    "The sentences in most of the cases are grammatically correct and these texts are also somewhat making sense. However, In some cases, the starting and the ending of the texts may seem to make it as an incomplete sentence . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Perplexity for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_perp = []\n",
    "\n",
    "for sent in test_sent_list:\n",
    "    ngrams = []\n",
    "    tokens = sent.split()\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = ' '.join(ngram[:-1])\n",
    "        last_token = str(ngram[-1])\n",
    "\n",
    "        if token_seq not in freq_count:\n",
    "            custom_perp.append(1)\n",
    "\n",
    "        elif last_token not in freq_count[token_seq]:\n",
    "            custom_perp.append(1)\n",
    "\n",
    "        else:\n",
    "            if N!=1:\n",
    "                choices = freq_count[token_seq].items()\n",
    "            else:\n",
    "                choices = freq_count[''].items()\n",
    "            pvals=[]\n",
    "            key_words=[]\n",
    "            total = sum(weight for choice, weight in choices)\n",
    "            custom_perp.append(freq_count[token_seq][last_token]/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(custom_perp, c):\n",
    "    for i in range(len(custom_perp)):\n",
    "        if custom_perp[i]==1:\n",
    "            custom_perp[i]= c\n",
    "    return custom_perp\n",
    "\n",
    "c = min(custom_perp)\n",
    "custom_perp = smoothing(custom_perp,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 1 gram:  378.04495410782584\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 2 gram:  171.27055085484562\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 3 gram:  75.76329766782628\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for 4 gram:  29.360625575730996\n"
     ]
    }
   ],
   "source": [
    "log_perp = abs(np.log(custom_perp))\n",
    "log_perp = sum(log_perp)/len(custom_perp)\n",
    "perplexity = np.exp(log_perp)\n",
    "print(\"Perplexity for\",N,\"gram: \",perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, InputLayer, Embedding, SimpleRNN, Dropout, LSTM\n",
    "from keras.activations import relu, softmax\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.utils as ku \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Data processing\n",
    "text = \" \".join(sent_list)\n",
    "text = text.replace(\"<s>\", \"\")\n",
    "text = text.replace(\"</s>\", \" \")\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for sw in stop_words:\n",
    "    text.replace(sw, '')\n",
    "\n",
    "new_sent_list = sent_tokenize(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(new_sent_list)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "for line in new_sent_list:\n",
    "    # https://keras.io/preprocessing/text/\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    count=0\n",
    "    for i in range(1, len(token_list)):\n",
    "#         count+=1\n",
    "#         if count>20:\n",
    "#           break\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "# https://keras.io/preprocessing/sequence/\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=12, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the RNN model\n",
    "\n",
    "def rnn_model(sequence_len, total_words):\n",
    "    input_len = sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=input_len))\n",
    "    model.add(SimpleRNN(150))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 11, 100)           884100    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 150)               37650     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8841)              1334991   \n",
      "=================================================================\n",
      "Total params: 2,256,741\n",
      "Trainable params: 2,256,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "129491/129491 [==============================] - 132s 1ms/step - loss: 6.1808 - acc: 0.0711\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.18076, saving model to RNN_weights_best.h5\n",
      "Epoch 2/5\n",
      "129491/129491 [==============================] - 131s 1ms/step - loss: 5.3336 - acc: 0.1409\n",
      "\n",
      "Epoch 00002: loss improved from 6.18076 to 5.33361, saving model to RNN_weights_best.h5\n",
      "Epoch 3/5\n",
      "129491/129491 [==============================] - 131s 1ms/step - loss: 4.9329 - acc: 0.1678\n",
      "\n",
      "Epoch 00003: loss improved from 5.33361 to 4.93295, saving model to RNN_weights_best.h5\n",
      "Epoch 4/5\n",
      "129491/129491 [==============================] - 135s 1ms/step - loss: 4.6399 - acc: 0.1893 3s -\n",
      "\n",
      "Epoch 00004: loss improved from 4.93295 to 4.63993, saving model to RNN_weights_best.h5\n",
      "Epoch 5/5\n",
      "129491/129491 [==============================] - 132s 1ms/step - loss: 4.3963 - acc: 0.2059\n",
      "\n",
      "Epoch 00005: loss improved from 4.63993 to 4.39628, saving model to RNN_weights_best.h5\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(predictors, label, test_size=0.2, random_state=1)\n",
    "model = rnn_model(12, total_words)\n",
    "n_epochs = 5\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"RNN_weights_best.h5\", monitor='loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history_rnn = model.fit(X_train, Y_train, batch_size=128, callbacks=callbacks_list, epochs=n_epochs, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text using RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(start_text, next_words, sequence_len, rnn_model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([start_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=sequence_len-1, padding='pre')\n",
    "        predicted = rnn_model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        start_text += \" \" + output_word\n",
    "    return start_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you should have been a big relationship with the world and i'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"you should\", 10, 12, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32373/32373 [==============================] - 18s 556us/step\n",
      "Test acuuracy:  0.18045902449802323\n",
      "Test Loss:  5.0735452228682005\n",
      "Perplexity:  159.7396372914065\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"Test acuuracy: \", scores[1])\n",
    "print(\"Test Loss: \", scores[0])\n",
    "print(\"Perplexity: \", np.exp(scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Texts using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you should have been a big relationship with the world and i\n",
      "it is a big thing we have a lot of money and\n",
      "that is going to be a wall and weâ€™re going to be\n",
      "i am a very good relationship with the people that are going\n",
      "you are going to be a lot of people that are going\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"you should\", 10, 12, model))\n",
    "print(generate_text(\"it is\", 10, 12, model))\n",
    "print(generate_text(\"that is\", 10, 12, model))\n",
    "print(generate_text(\"i am\", 10, 12, model))\n",
    "print(generate_text(\"you are\", 10, 12, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readibility:\n",
    "Grammer of LSTM is also correct most of times. However, Since the sentence length is 10, it seems incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Data processing\n",
    "text = \" \".join(sent_list)\n",
    "text = text.replace(\"<s>\", \"\")\n",
    "text = text.replace(\"</s>\", \" \")\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for sw in stop_words:\n",
    "    text.replace(sw, '')\n",
    "\n",
    "new_sent_list = sent_tokenize(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(new_sent_list)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "\n",
    "for line in new_sent_list:\n",
    "\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    count=0\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=12, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "\n",
    "def lstm_model(sequence_len, total_words):\n",
    "    input_len = sequence_len - 1\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=input_len))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 11, 100)           884100    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 150)               150600    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8841)              1334991   \n",
      "=================================================================\n",
      "Total params: 2,369,691\n",
      "Trainable params: 2,369,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "129491/129491 [==============================] - 175s 1ms/step - loss: 6.5989 - acc: 0.0336\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.59889, saving model to LSTM_weights_best.h5\n",
      "Epoch 2/5\n",
      "129491/129491 [==============================] - 170s 1ms/step - loss: 6.4765 - acc: 0.0335\n",
      "\n",
      "Epoch 00002: loss improved from 6.59889 to 6.47651, saving model to LSTM_weights_best.h5\n",
      "Epoch 3/5\n",
      "129491/129491 [==============================] - 176s 1ms/step - loss: 6.4685 - acc: 0.0331\n",
      "\n",
      "Epoch 00003: loss improved from 6.47651 to 6.46848, saving model to LSTM_weights_best.h5\n",
      "Epoch 4/5\n",
      "129491/129491 [==============================] - 166s 1ms/step - loss: 6.3101 - acc: 0.0469\n",
      "\n",
      "Epoch 00004: loss improved from 6.46848 to 6.31012, saving model to LSTM_weights_best.h5\n",
      "Epoch 5/5\n",
      "129491/129491 [==============================] - 167s 1ms/step - loss: 5.7259 - acc: 0.1103\n",
      "\n",
      "Epoch 00005: loss improved from 6.31012 to 5.72589, saving model to LSTM_weights_best.h5\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(predictors, label, test_size=0.2, random_state=1)\n",
    "model = lstm_model(12, total_words)\n",
    "n_epochs = 5\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"LSTM_weights_best.h5\", monitor='loss', verbose=1, \n",
    "                                 save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=128, callbacks=callbacks_list, epochs=n_epochs, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_text, next_words, sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([start_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "  \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        start_text += \" \" + output_word\n",
    "    return start_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you should be a lot of the country and i have to'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"you should\", 10, 12, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Texts using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it takes a lot of people that are going to be a\n",
      "he is a great guy and i said i donâ€™t want to\n",
      "but she was a very very successful person to be a very\n",
      "they go to the best people in the world and they said\n",
      "that place is a big problem and the other day and we\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"it takes\", 10, 12, model))\n",
    "print(generate_text(\"he is\", 10, 12, model))\n",
    "print(generate_text(\"but she\", 10, 12, model))\n",
    "print(generate_text(\"they go\", 10, 12, model))\n",
    "print(generate_text(\"that place\", 10, 12, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readibility:\n",
    "Grammer of LSTM is also correct most of times. However, Since the sentence length is 10, it seems incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preplexity and Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32373/32373 [==============================] - 15s 478us/step\n",
      "Test acuuracy:  0.1257529422708464\n",
      "Test Loss:  5.667928131672545\n",
      "Perplexity:  289.43424310305045\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"Test acuuracy: \", scores[1])\n",
    "print(\"Test Loss: \", scores[0])\n",
    "print(\"Perplexity: \", np.exp(scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural approach performs better than the Classical one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the perplexity of Neural approach is better as compared to the bigrams and unigrams. However, even though, it's perplexity is more than that of tri and quadgrams, it's main reason would be because it was trained on a small number of epochs. The main reason, I think why Neural approach performed better is because they are good at approximating non-linear functions (in our case it is trying to figure out a word given previous words). The above was a very simple Neural approach, However, if we increase the complexity of our Neural model (increasing neurons, adding additional layers) which is not easily possible for n-gram, we can achieve great results.\n",
    "Classical approach are also restricted to just taking into account n-previous grams, However, in case of Neural approach, it can be much larger. I think, This also played a crucial role in reducing it's perplexity.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

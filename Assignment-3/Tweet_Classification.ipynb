{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import keras.utils as ku\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, InputLayer, Embedding, SimpleRNN, Dropout, LSTM, Bidirectional, Input, Conv1D, MaxPooling1D, Concatenate, Flatten\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "from keras.activations import relu, softmax\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.txt\n",
      "Exampleno. : 0\n",
      "Exampleno. : 50000\n",
      "Exampleno. : 100000\n",
      "Exampleno. : 150000\n",
      "Exampleno. : 200000\n",
      "Exampleno. : 250000\n",
      "Exampleno. : 300000\n",
      "Exampleno. : 350000\n",
      "Exampleno. : 400000\n",
      "\n",
      " Test.txt\n",
      "Exampleno. : 0\n",
      "Exampleno. : 50000\n"
     ]
    }
   ],
   "source": [
    "Y,X = [], []\n",
    "lang=[]\n",
    "lang_str =[]\n",
    "cal_str =[]\n",
    "\n",
    "def Preprocessing(f, num_lines):\n",
    "    Y=[]\n",
    "    X=[]\n",
    "    lang=[]\n",
    "    lang_str =[]\n",
    "    cal_str =[]\n",
    "    bad_list = ['https']\n",
    "    for i in range(num_lines):\n",
    "        if i%50000==0:\n",
    "            print(\"Exampleno. :\",i)\n",
    "    #         print(line_d)\n",
    "        line = f.readline()\n",
    "        line_d = line.split()\n",
    "        if len(line_d)==3:\n",
    "            start=True\n",
    "            Y.append(line_d[2])\n",
    "        elif len(line_d)== 0 or len(line_d) == 1:\n",
    "            pass\n",
    "        elif line_d[0]=='@' or line_d[0]=='#':\n",
    "            line = f.readline()\n",
    "            pass\n",
    "        elif len(line_d[0])==1:\n",
    "            pass\n",
    "        elif not line_d[0].isalpha():\n",
    "            pass\n",
    "        elif line_d[0]in bad_list:\n",
    "            pass\n",
    "        else:\n",
    "            if line_d[-1]=='O':\n",
    "                pass\n",
    "            elif start:\n",
    "                X.append(cal_str)\n",
    "                lang.append(lang_str)\n",
    "                cal_str, lang_str = [], []\n",
    "                cal_str.append(line_d[0])\n",
    "                lang_str.append(line_d[1])\n",
    "                start=False\n",
    "            else:\n",
    "                cal_str.append(line_d[0])\n",
    "                lang_str.append(line_d[1])\n",
    "    return X, Y, lang\n",
    "print('Train.txt')\n",
    "f = open(\"raw_data/nlp_assignment_3/train.txt\", \"r\",encoding=\"utf8\")\n",
    "test_num_lines = sum(1 for line in f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"raw_data/nlp_assignment_3/train.txt\", \"r\",encoding=\"utf8\")\n",
    "X, Y, lang = Preprocessing(f, test_num_lines)\n",
    "f.close()\n",
    "\n",
    "print('\\n Test.txt')\n",
    "f = open(\"raw_data/nlp_assignment_3/test.txt\", \"r\",encoding=\"utf8\")\n",
    "test_num_lines = sum(1 for line in f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"raw_data/nlp_assignment_3/test.txt\", \"r\",encoding=\"utf8\")\n",
    "X_test, Y_test, lang_test = Preprocessing(f, test_num_lines)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 15130\n",
      "Test data: 1868\n"
     ]
    }
   ],
   "source": [
    "# correct Indexing\n",
    "X.pop(0)\n",
    "Y.pop(-1)\n",
    "lang.pop(0)\n",
    "lang_test.pop(0)\n",
    "X_test.pop(0)\n",
    "Y_test.pop(-1)\n",
    "print(\"Training data:\",len(X))\n",
    "print(\"Test data:\",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming sentences\n",
    "sent_list = []\n",
    "test_sent_list=[]\n",
    "for line in X:\n",
    "    sent_list.append(' '.join(line).lower())\n",
    "for line in X_test:\n",
    "    test_sent_list.append(' '.join(line).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt love looks good on maddie ako lang ba yung sobrang masaya kasi may zolo sya before with the past medyo lowkey',\n",
       " 'ye ye ye we gonna start another june on sour note uhhhh yes no yes no yes',\n",
       " 'of lyching mein kahna nae chahta qki mere yaha btay tco',\n",
       " 'caring bohot jyada caring courier wale bsdk ke sign bhi khud hi krlete mera',\n",
       " 'what nonesense kabhi baymani per bani team kamiyab nahi ho sakti jo log apnay liy co',\n",
       " 'best of luck sir world cup ke liye bhot bhot subhkamnaye',\n",
       " 'yes great dialogues in that one also chupke chupke over chhaddabeshi all except ke sholay co',\n",
       " 'desh bhakti baat wahi samajh sakte hai jo khud deshbhakt wo log to sirf bharat tere tukde hong co rmdzsiwsgt',\n",
       " 'pakistani team ne effort ki aagey allah ki marziiiiiiiiiiiii',\n",
       " 'nsharif kiya tum apne baap ki oulad nahi kyun usy bay yaro madadgar chora hua kiya wo co']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word Index: 34403\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sent_list+ test_sent_list)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"Size of word Index:\", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data prepared!\n",
      "Test Data prepared!\n"
     ]
    }
   ],
   "source": [
    "train_input_sequences = []\n",
    "test_input_sequences = []\n",
    "\n",
    "for line in sent_list:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    train_input_sequences.append(token_list)\n",
    "for line in test_sent_list:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    test_input_sequences.append(token_list)\n",
    "\n",
    "max_sequence_len = max(max([len(x) for x in train_input_sequences]), max([len(x) for x in test_input_sequences]))\n",
    "train_input_sequences = np.array(pad_sequences(train_input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "test_input_sequences = np.array(pad_sequences(test_input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# 001 -> Positive\n",
    "# 010 -> Neutral\n",
    "# 100 -> Negative\n",
    "train_label =[]\n",
    "test_label =[]\n",
    "for i in Y:\n",
    "    if i == 'negative':train_label.append([1,0,0])\n",
    "    elif i == 'positive':train_label.append([0,0,1])\n",
    "    else:train_label.append([0,1,0])\n",
    "train_label = np.asarray(train_label)\n",
    "print(\"Train Data prepared!\")\n",
    "for i in Y_test:\n",
    "    if i == 'negative':test_label.append([1,0,0])\n",
    "    elif i == 'positive':test_label.append([0,0,1])\n",
    "    else:test_label.append([0,1,0])\n",
    "test_label = np.asarray(test_label)\n",
    "print(\"Test Data prepared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hindi words replaced in train  98728\n",
      "Number of Hindi words replaced in test  12013\n",
      "Total word vectors:  400000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Source: Obtained from Github:precog-iiitd /mind-your-language-aaai\n",
    "# Translation of some Hindi words (written in English) to English words\n",
    "json1_file = open('dictionary_hinglish_transliteration.json')\n",
    "hinglish = json1_file.read()\n",
    "hinglish_dict = json.loads(hinglish)\n",
    "hindi_words = hinglish_dict.keys()\n",
    "\n",
    "# Source: Obtained from Github:HarshTrivedi /HinglishSentiment\n",
    "Hindi_stopwords = ['ke', 'ka', 'ek', 'mein', 'ki', 'hai', 'yah', 'aur', 'se', 'hain', 'ko', 'par', 'iss', 'hota', 'jo', 'kar', 'me', 'gaya', 'karne', 'kiya', 'liye', 'apne', 'ne', 'bani', 'nahi', 'toh', 'hi', 'ya', 'avam', 'diya', 'ho', 'iska', 'tha', 'dhvara', 'hua', 'tak', 'saath', 'karna', 'vaale', 'baad', 'liya', 'aap', 'kuchh', 'sakte', 'kisi', 'ye', 'iska', 'sabse', 'ismein', 'the', 'do', 'hone', 'vah', 've', 'karte', 'bahut', 'kaha', 'varg', 'kai', 'karein', 'hoti', 'apni', 'unke', 'thi', 'yadi', 'hui', 'jaa', 'na', 'ise', 'kehte', 'kahte', 'jab', 'hote', 'koi', 'hue', 'va', 'abhi', 'jaise', 'sabhi', 'karta', 'unki', 'tarah', 'uss', 'aadi', 'kul', 'raha', 'iski', 'sakta', 'rahe', 'unka', 'issi', 'rakhein', 'apna', 'pe', 'uske']\n",
    "count1=0\n",
    "count=0\n",
    "count2=0\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X[i])):\n",
    "        text = X[i][j].lower()\n",
    "        if text in hindi_words:\n",
    "            count+=1\n",
    "            X[i][j] = hinglish_dict[text]\n",
    "\n",
    "print(\"Number of Hindi words replaced in train \",count)\n",
    "\n",
    "count1=0\n",
    "count=0\n",
    "count2=0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[i])):\n",
    "        text = X_test[i][j].lower()\n",
    "        if text in hindi_words:\n",
    "            count+=1\n",
    "            X_test[i][j] = hinglish_dict[text]\n",
    "        elif lang_test[i][j]=='Hin':\n",
    "            count2+=1\n",
    "            X_test[i][j] = ''        \n",
    "print(\"Number of Hindi words replaced in test \",count)\n",
    "\n",
    "\n",
    "# Glove Embeddings\n",
    "EMBEDDING_DIM = 100\n",
    "embeddings_index = {}\n",
    "word_index = tokenizer.word_index\n",
    "EMBEDDING_DIM = 100\n",
    "glove_file = open('.glove.6B/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "glove_file.close()\n",
    "\n",
    "print('Total word vectors: ',len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_Model(max_len):\n",
    "    sequence_input = Input(shape=(max_len,))\n",
    "    embedded_sequences = Embedding(total_words, 100)(sequence_input)\n",
    "    lstm1 = Bidirectional(LSTM(8,dropout=0.3,recurrent_dropout=0.3,return_sequences=True))(embedded_sequences)\n",
    "    \n",
    "    class_block, filter_sizes = [], [3,4,6,7]\n",
    "    for diff_filters in filter_sizes:\n",
    "        conv_l1 = Conv1D(filters=12,kernel_size=1,\n",
    "                        activation='relu',)(lstm1)\n",
    "        conv_l2 = Conv1D(filters=12,kernel_size=diff_filters,\n",
    "                           activation='relu',kernel_regularizer=regularizers.l2(0.02))(conv_l1)\n",
    "        class_block.append(conv_l2)\n",
    "\n",
    "    merge_layer = Concatenate(axis=1)(class_block)\n",
    "#     pool_1 = MaxPooling1D(3)(merge_layer)\n",
    "    l_drop1 = Dropout(0.4)(merge_layer)\n",
    "    flat1 = Flatten()(l_drop1)\n",
    "    dense_l1 = Dense(24, activation='relu',kernel_regularizer=regularizers.l2(0.02))(flat1)\n",
    "\n",
    "    out = Dense(3, activation='softmax')(dense_l1)\n",
    "    model = Model(sequence_input, out)\n",
    "    return model\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 31)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 31, 100)      3440300     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 31, 16)       6976        embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, 31, 12)       204         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, 31, 12)       204         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, 31, 12)       204         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, 31, 12)       204         bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, 29, 12)       444         conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, 28, 12)       588         conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, 26, 12)       876         conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, 25, 12)       1020        conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 108, 12)      0           conv1d_90[0][0]                  \n",
      "                                                                 conv1d_92[0][0]                  \n",
      "                                                                 conv1d_94[0][0]                  \n",
      "                                                                 conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 108, 12)      0           concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 1296)         0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 24)           31128       flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 3)            75          dense_23[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,482,223\n",
      "Trainable params: 3,482,223\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_Model = Custom_Model(max_sequence_len)\n",
    "# adadelta = optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
    "# keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "custom_Model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc', recall_m, precision_m, f1_m])\n",
    "custom_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15130 samples, validate on 1868 samples\n",
      "Epoch 1/8\n",
      "15130/15130 [==============================] - 82s 5ms/step - loss: 0.9819 - acc: 0.5454 - recall_m: 0.3231 - precision_m: 0.5857 - f1_m: 0.3997 - val_loss: 0.9689 - val_acc: 0.5589 - val_recall_m: 0.3542 - val_precision_m: 0.6046 - val_f1_m: 0.4374\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.96886, saving model to try_weights.h5\n",
      "Epoch 2/8\n",
      "15130/15130 [==============================] - 80s 5ms/step - loss: 0.8299 - acc: 0.6408 - recall_m: 0.5632 - precision_m: 0.6729 - f1_m: 0.6086 - val_loss: 1.0331 - val_acc: 0.5691 - val_recall_m: 0.5064 - val_precision_m: 0.5876 - val_f1_m: 0.5409\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.96886\n",
      "Epoch 3/8\n",
      "15130/15130 [==============================] - 80s 5ms/step - loss: 0.7692 - acc: 0.6827 - recall_m: 0.6362 - precision_m: 0.7027 - f1_m: 0.6654 - val_loss: 0.9584 - val_acc: 0.5926 - val_recall_m: 0.5545 - val_precision_m: 0.6035 - val_f1_m: 0.5765\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.96886 to 0.95838, saving model to try_weights.h5\n",
      "Epoch 4/8\n",
      "15130/15130 [==============================] - 81s 5ms/step - loss: 0.7024 - acc: 0.7246 - recall_m: 0.6987 - precision_m: 0.7415 - f1_m: 0.7180 - val_loss: 1.0158 - val_acc: 0.5910 - val_recall_m: 0.5684 - val_precision_m: 0.6007 - val_f1_m: 0.5830\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.95838\n",
      "Epoch 5/8\n",
      "15130/15130 [==============================] - 82s 5ms/step - loss: 0.6354 - acc: 0.7655 - recall_m: 0.7443 - precision_m: 0.7771 - f1_m: 0.7591 - val_loss: 1.1043 - val_acc: 0.5851 - val_recall_m: 0.5737 - val_precision_m: 0.5954 - val_f1_m: 0.5837\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.95838\n",
      "Epoch 6/8\n",
      "15130/15130 [==============================] - 81s 5ms/step - loss: 0.5593 - acc: 0.8025 - recall_m: 0.7889 - precision_m: 0.8133 - f1_m: 0.8001 - val_loss: 1.0194 - val_acc: 0.5948 - val_recall_m: 0.5705 - val_precision_m: 0.6023 - val_f1_m: 0.5849\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.95838\n",
      "Epoch 7/8\n",
      "15130/15130 [==============================] - 81s 5ms/step - loss: 0.4996 - acc: 0.8323 - recall_m: 0.8207 - precision_m: 0.8418 - f1_m: 0.8304 - val_loss: 1.0013 - val_acc: 0.5990 - val_recall_m: 0.5732 - val_precision_m: 0.6044 - val_f1_m: 0.5872\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.95838\n",
      "Epoch 8/8\n",
      "15130/15130 [==============================] - 82s 5ms/step - loss: 0.4515 - acc: 0.8542 - recall_m: 0.8425 - precision_m: 0.8614 - f1_m: 0.8512 - val_loss: 1.1396 - val_acc: 0.5974 - val_recall_m: 0.5780 - val_precision_m: 0.6069 - val_f1_m: 0.5912\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.95838\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('try_weights.h5',monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "n_epochs=8\n",
    "history_custom = custom_Model.fit(train_input_sequences, train_label, batch_size=8, callbacks=callbacks_list, validation_data=(test_input_sequences, test_label), epochs=n_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63       532\n",
      "           1       0.58      0.50      0.54       754\n",
      "           2       0.60      0.67      0.63       582\n",
      "\n",
      "    accuracy                           0.60      1868\n",
      "   macro avg       0.60      0.61      0.60      1868\n",
      "weighted avg       0.60      0.60      0.59      1868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = np.argmax(custom_Model.predict(test_input_sequences), axis=1)\n",
    "report = classification_report(np.argmax(test_label, axis=1),y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
